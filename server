"def auto_login(user_id,user_pw)"""
import getpass
import json
from selenium import webdriver

'''
건국대학교 e캠퍼스 자동로그인  
'''

options = webdriver.ChromeOptions()
crawl_api = 'http://ecampus.konkuk.ac.kr/ilos/main/main_form.acl'
# headless 옵션 설정 : 개발환경이 리눅스라면 아래 두가지는 포함
options.add_argument('headless')
options.add_argument("no-sandbox")

# 브라우저 사이즈 : 현재 창을 열지 않는 방식으로 구현
# options.add_argument('window-size=800,600')

user_id = input('아이디를 입력하세요: ')
user_pw = getpass.getpass('비밀번호를 입력하세요: ')

# 드라이버 위치 경로 입력
driver = webdriver.Chrome()

# url을 이용하여 브라우저로 접속
driver.get('https://ecampus.konkuk.ac.kr/ilos/main/member/login_form.acl')

# 대기시간 부여
driver.implicitly_wait(3)

driver.find_element_by_id('usr_id').send_keys(user_id)
driver.find_element_by_id('usr_pwd').send_keys(user_pw)
driver.find_element_by_xpath('//*[@id="login_btn"]').click()

# 대기시간 부여
driver.implicitly_wait(5)

_cookies = driver.get_cookies()
cookie_dict = {}
for cookie in _cookies:
    cookie_dict[cookie['name']] = cookie['value']

session = requests.Session()
headers = {'User-Agent': 'Mozila/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36(KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36 Edg/96.0.1054.62'}

session.headers.update(headers)
session.cookies.update(cookie_dict)
headers = {'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8'}

res = session.post(crawl_api, headers=headers, data=data)
json_obj = json.loads(res.text)
print(json_obj)

#driver.quit()
 
 
 #이 아래부터는 강의 리스트 및 강의 자료 다운로드 코드입니다.
import requests
from bs4 import BeautifulSoup
url = 'https://ecampus.konkuk.ac.kr/ilos/main/member/login_form.acl'
data = {
    'usr_id':'jeh8521',
    'usr_pwd':'shhs20320!@'
}
s = requests.Session()
response = s.post(url, data=data)
response.status_code
soup = BeautifulSoup(response.text)
mypage='http://ecampus.konkuk.ac.kr/ilos/st/course/submain_form.acl'
resp = s.get(mypage)
soup = BeautifulSoup(resp.text)
td = soup.select('span.welcome_subject')
td = soup.select('em.sub_open')

import os
import re
from urllib import request
from urllib.error import HTTPError
from bs4 import BeautifulSoup
import requests
​
overlap=[]
url = ''  #반복되는 곳 사이트 몸통
site = ''   #가져올 사이트 앞부분
rec = "" #반복되는 부분 뒷부분
dl = ""   #클릭해서 다운 받는 부분
​
def get_download(url, fname, directory):
   try:
       os.chdir(directory)
       request.urlretrieve(url, fname)
       print('다운로드 완료\n')
   except HTTPError as e:
       print('error')
       return
​
def downSearch(getDLATag):
   for getDLLink in getDLATag:
       try:
           if dl in getDLLink.get('href'):
               print("다운로드 링크 : {}".format(site) + getDLLink.get('href'))
               accessDLUrl = site + getDLLink.get('href')
               fileOriginalNM = re.sub('<.+?>', '', str(getDLLink), 0).strip().replace('_', ' ')
               fileNM = "[KAMIS주간동향] " + fileOriginalNM
               path = "D:\\KAMIS\\"
               if os.path.isfile(path + fileNM):
                   print("다운로드 실패 : 동일 파일 존재\n")
               else:
                   get_download(accessDLUrl, fileNM, path)
       except:pass
​
def Search(getA):
   for getLink in getA:
       data = getLink.get('href')
       try:
           if rec in getLink.get("href"):
               if len(data) >= 100 and data not in overlap:
                   overlap.append(data)
                   accessUrl = site + getLink.get("href")
                   r = requests.get(accessUrl)
                   soup = BeautifulSoup(r.text, "html.parser")
                   getDLATag = soup.find_all("a")
                   downSearch(getDLATag)
​
               elif len(data) >= 85 and data not in overlap:
                   overlap.append(data)
                   accessUrl = site + getLink.get("href")
                   r = requests.get(accessUrl)
                   soup = BeautifulSoup(r.text, "html.parser")
                   getDLATag = soup.find_all("a")
                   Search(getDLATag)
                   # Search(getDLATag,depth+1)
       except:pass
​
# request 모듈을 사용하여 웹 페이지의 내용을 가져온다
r = requests.get(url)
print("자료실 요청 : ", r)
​
# beautiful soup 초기화
soup = BeautifulSoup(r.text, "html.parser")
# 태그로 찾기 (모든 항목)
getA = soup.find_all("a")
Search(getA)
​
